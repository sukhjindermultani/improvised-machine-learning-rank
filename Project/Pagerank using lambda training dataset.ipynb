{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Return the list of the dcg by browsing the list of documensts\n",
    "def dcg(scores):\n",
    "    return np.sum([(np.power(2, scores[i]) - 1) / np.log2(i + 2) for i in range(len(scores))]) \n",
    "\n",
    "# Return the list of the dcg by browsing the list of documents \n",
    "#(and truncating, that is, taking only the first k documents)\n",
    "def dcg_k(scores, k):\n",
    "    return np.sum([(np.power(2, scores[i]) - 1) / np.log2(i + 2) for i in range(len(scores[:k]))])\n",
    "\n",
    "# Return the list of the dcg by browsing the list of documents in the descending order of the scores \n",
    "# In other words, the documents are well classified\n",
    "def ideal_dcg(scores):\n",
    "    scores = [score for score in sorted(scores)[::-1]]\n",
    "    return dcg(scores)\n",
    "\n",
    "# Return the list of the dcg by browsing the list of documents in the descending order of the scores\n",
    "#(and truncating, that is, taking only the first k documents)\n",
    "def ideal_dcg_k(scores, k):\n",
    "    scores = [score for score in sorted(scores)[::-1]]\n",
    "    return dcg_k(scores, k)\n",
    "\n",
    "# Returns the dcg of two documents \n",
    "def single_dcg(scores, i, j):\n",
    "    return (np.power(2, scores[i]) - 1) / np.log2(j + 2)\n",
    "\n",
    "# Calculate the lambdas (which will allow the gradient descent)\n",
    "def compute_lambda(args):\n",
    "     # We take as argument the list of real scores, the list of predicted scores, \n",
    "    # pairs of documents such as document i has a higher score than document j,\n",
    "    # the list of documents idcg and the list of requests  \n",
    "    true_scores, predicted_scores, good_ij_pairs, idcg, query_key = args  \n",
    "    # Count the number of documents\n",
    "    num_docs = len(true_scores)\n",
    "    # Sort the places of the documents in descending order of the scores\n",
    "    # Otherwise we have first place in the list of the highest score document etc.\n",
    "    sorted_indexes = np.argsort(predicted_scores)[::-1]\n",
    "    # Sort the places of the documents in ascending order of the scores\n",
    "    # Otherwise we have first place in the list of document at the lowest score etc.\n",
    "    rev_indexes = np.argsort(sorted_indexes)\n",
    "    # We take the real scores of the documents sorted according to the order of the scores\n",
    "    true_scores = true_scores[sorted_indexes]\n",
    "    # We take the predicted scores of the documents sorted according to the order of the scores\n",
    "    predicted_scores = predicted_scores[sorted_indexes]\n",
    "    # We create an empty vector of size the number of documents for lambdas\n",
    "    lambdas = np.zeros(num_docs)\n",
    "    # We create an empty vector of the size the number of documents for the weights\n",
    "    w = np.zeros(num_docs)\n",
    "    # We create a dictionary so the keys are the document pairs and the dcg values between the two documents \n",
    "    single_dcgs = {}\n",
    "    # We go through the pairs of documents so the document i has a score higher than the document j\n",
    "    # We add to the dictator the cdg between i and j and between j and i, then between i and i and j and j if they do not already exist\n",
    "    for i,j in good_ij_pairs:\n",
    "        if (i,i) not in single_dcgs:\n",
    "            single_dcgs[(i,i)] = single_dcg(true_scores, i, i)\n",
    "        single_dcgs[(i,j)] = single_dcg(true_scores, i, j)\n",
    "        if (j,j) not in single_dcgs:\n",
    "            single_dcgs[(j,j)] = single_dcg(true_scores, j, j)\n",
    "        single_dcgs[(j,i)] = single_dcg(true_scores, j, i) \n",
    "    for i,j in good_ij_pairs:\n",
    "        # Calculation of the NDCG for each pair (differences of dcg on dcg of i (the largest dcg))\n",
    "        z_ndcg = abs(single_dcgs[(i,j)] - single_dcgs[(i,i)] + single_dcgs[(j,i)] - single_dcgs[(j,j)]) / idcg\n",
    "        # Function defined in the article for the calculation of lambdas\n",
    "        rho = 1 / (1 + np.exp(predicted_scores[i] - predicted_scores[j]))\n",
    "        # rho_complement ?\n",
    "        rho_complement = 1.0 - rho\n",
    "        # Multiply the NDCG for the pair i, j by rho\n",
    "        lambda_val = z_ndcg * rho\n",
    "        # We sum lambda i, j to obtain lambda i and lambda j\n",
    "        lambdas[i] += lambda_val\n",
    "        lambdas[j] -= lambda_val\n",
    "        # We update the weights too \n",
    "        w_val = rho * rho_complement * z_ndcg\n",
    "        w[i] += w_val\n",
    "        w[j] += w_val\n",
    "    # We return the lambdas and weights for the sorted documents in ascending order of the scores\n",
    "    # In other words, first, we have the document with the lowest score\n",
    "    return lambdas[rev_indexes], w[rev_indexes], query_key\n",
    "\n",
    "def group_queries(training_data, qid_index):\n",
    "    # We create a dictionary whose keys are the queries and the values the lists of places of the documents in the list\n",
    "    # documents used in the train database (list of lists containing the query the score and the features associated with the documents)\n",
    "    query_indexes = {}\n",
    "    index = 0\n",
    "    for record in training_data:\n",
    "        query_indexes.setdefault(record[qid_index], [])\n",
    "        query_indexes[record[qid_index]].append(index)\n",
    "        index += 1\n",
    "    return query_indexes\n",
    " \n",
    "# At the input of the function, we have a list of score lists according to the request\n",
    "def get_pairs(scores):\n",
    "    #We define a list \n",
    "    query_pair = []\n",
    "    # Here we go through the lists in the list \n",
    "    for query_scores in scores:\n",
    "        # We sort the scores in descending order (highest scores in premiere)\n",
    "        temp = sorted(query_scores, reverse=True)\n",
    "        pairs = []\n",
    "        for i in range(len(temp)):\n",
    "            for j in range(len(temp)):\n",
    "                if temp[i] > temp[j]:\n",
    "                    pairs.append((i,j))\n",
    "        # We add to the final list \n",
    "        query_pair.append(pairs)\n",
    "    return query_pair\n",
    "\n",
    "# We define the LambdaMART class\n",
    "class LambdaMART:\n",
    "\n",
    "    #We fix 5 trees and a learning rate of 0.1 .. we can test other learning rate after !!\n",
    "    def __init__(self, training_data=None, number_of_trees=5, learning_rate=0.1):\n",
    "\n",
    "        self.training_data = training_data\n",
    "        self.number_of_trees = number_of_trees\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "    \n",
    "    # The function fit allows to fitter the lambdas tree\n",
    "    def fit(self):\n",
    "        # We define as many predicted scores as lines in the training set\n",
    "        predicted_scores = np.zeros(len(self.training_data))\n",
    "        # On définit notre dictionnaire dont les clés sont les requêtes et les valeurs les places des documents\n",
    "        # in the list of documents (the 1 because the second value of the list for each document has the second place the request)\n",
    "        query_indexes = group_queries(self.training_data, 1)\n",
    "        # We retrieve the list of requests\n",
    "        query_keys = query_indexes.keys()\n",
    "        # We obtain here a list of lists of scores, by request\n",
    "        true_scores = [self.training_data[query_indexes[query], 0] for query in query_keys]\n",
    "        # We obtain here the pairs of documents for each list of scores, so for each request\n",
    "        good_ij_pairs = get_pairs(true_scores)\n",
    "        # Here we obtain the set of feature vectors for documents\n",
    "        tree_data = pd.DataFrame(self.training_data[:, 2:7])\n",
    "        # We obtain here all the scores for the documents (the labels)\n",
    "        labels = self.training_data[:, 0]\n",
    "        # The list of dcg is calculated for the lists of scores ranked in descending order (ideal ranking)\n",
    "        # We get a list of lists again\n",
    "        idcg = [ideal_dcg(scores) for scores in true_scores]\n",
    "        # We go through the number of trees \n",
    "        for k in range(self.number_of_trees):\n",
    "            # We create as many lambdas as we have lines in our training set\n",
    "            lambdas = np.zeros(len(predicted_scores))\n",
    "            # We create as much weight as we have lines in our training set\n",
    "            w = np.zeros(len(predicted_scores))\n",
    "            # Here we obtain a list of predicted scores lists, by query\n",
    "            pred_scores = [predicted_scores[query_indexes[query]] for query in query_keys]\n",
    "            # pool allows to parrallelize already on the 4 hearts of the computer, by request \n",
    "            # Here, we obtain the lambdas, the weights for the documents of each request\n",
    "            # In input of compute_lambda, one has the lists of the real scores for the docs of each request,\n",
    "            # lists of predicted scores, lists of pairs of docs such as the first element has a higher score than the second,\n",
    "            # the list of the dcg of the ideal classification of the documents, by request\n",
    "            for lambda_val, w_val, query_key in map(compute_lambda, zip(true_scores, pred_scores, good_ij_pairs, idcg, query_keys)):\n",
    "                # Here, we obtain the list of the places of the documents associated with the request \n",
    "                indexes = query_indexes[query_key]\n",
    "                # We get the lambdas of the docs associated with the input request, we enter the final lambda vector\n",
    "                lambdas[indexes] = lambda_val\n",
    "                # We obtain the weights of the docs associated with the input request, we enter the vector final weight\n",
    "                w[indexes] = w_val\n",
    "            # Implementation sklearn of the three\n",
    "            tree = DecisionTreeRegressor(max_depth=50)\n",
    "            # We made the lambdas tree\n",
    "            tree.fit(self.training_data[:,2:], lambdas)\n",
    "            # We add the tree to our list of trees\n",
    "            self.trees.append(tree)\n",
    "            # We predict thanks to our tree and our features (the predict function is below)\n",
    "            # Calculate the prediction for each document thanks to our previous trees and the new\n",
    "            prediction = tree.predict(self.training_data[:,2:])\n",
    "            # We update our predictions using our recent prediciton and learning_rate\n",
    "            predicted_scores += prediction * self.learning_rate\n",
    "\n",
    "    \n",
    "    def predict(self, data):\n",
    "        data = np.array(data)\n",
    "        query_indexes = group_queries(data, 0)\n",
    "        predicted_scores = np.zeros(len(data))\n",
    "        for query in query_indexes:\n",
    "            results = np.zeros(len(query_indexes[query]))\n",
    "            for tree in self.trees:\n",
    "                results += self.learning_rate * tree.predict(data[query_indexes[query], 1:])\n",
    "            predicted_scores[query_indexes[query]] = results\n",
    "        return predicted_scores\n",
    "\n",
    "    # Validate function that predicts on the test base according to all the trees we have built, which also calculates the NDCG average\n",
    "    def validate(self, data, k):\n",
    "        data = np.array(data)\n",
    "        query_indexes = group_queries(data, 1)\n",
    "        average_ndcg = []\n",
    "        predicted_scores = np.zeros(len(data))\n",
    "        for query in query_indexes:\n",
    "            results = np.zeros(len(query_indexes[query]))\n",
    "            for tree in self.trees:\n",
    "                results += self.learning_rate * tree.predict(data[query_indexes[query], 2:])\n",
    "            predicted_sorted_indexes = np.argsort(results)[::-1]\n",
    "            t_results = data[query_indexes[query], 0]\n",
    "            t_results = t_results[predicted_sorted_indexes]\n",
    "            predicted_scores[query_indexes[query]] = results\n",
    "            dcg_val = dcg_k(t_results, k)\n",
    "            idcg_val = ideal_dcg_k(t_results, k)\n",
    "            ndcg_val = (dcg_val / idcg_val)\n",
    "            average_ndcg.append(ndcg_val)\n",
    "        average_ndcg = np.nanmean(average_ndcg)\n",
    "        return average_ndcg, predicted_scores\n",
    "\n",
    "def group_queries(training_data, qid_index):\n",
    "    # We create a dictionary whose keys are the queries and the values the lists of places of the documents in the list\n",
    "    # documents used in the train database (list of lists containing the query the score and the features associated with the documents)\n",
    "    query_indexes = {}\n",
    "    index = 0\n",
    "    for record in training_data:\n",
    "        query_indexes.setdefault(record[qid_index], [])\n",
    "        query_indexes[record[qid_index]].append(index)\n",
    "        index += 1\n",
    "    return query_indexes\n",
    " \n",
    "# At the input of the function, we have a list of score lists according to the request\n",
    "def get_pairs(scores):\n",
    "    #At the input of the function, we have a list of score lists according to the request\n",
    "    query_pair = []\n",
    "    # Here we go through the lists in the list \n",
    "    for query_scores in scores:\n",
    "        # We sort the scores in descending order (highest scores in premiere)\n",
    "        temp = sorted(query_scores, reverse=True)\n",
    "        pairs = []\n",
    "        for i in range(len(temp)):\n",
    "            for j in range(len(temp)):\n",
    "                if temp[i] > temp[j]:\n",
    "                    pairs.append((i,j))\n",
    "        # We add to the final list \n",
    "        query_pair.append(pairs)\n",
    "    return query_pair\n",
    "\n",
    "# We define the LambdaMART class\n",
    "class LambdaMART:\n",
    "\n",
    "    #We fix 5 trees and a learning rate of 0.1 .. we can test other learning rate after !!\n",
    "    def __init__(self, training_data=None, number_of_trees=5, learning_rate=0.1):\n",
    "\n",
    "        self.training_data = training_data\n",
    "        self.number_of_trees = number_of_trees\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "    \n",
    "    # The function fit allows to fitter the lambdas tree\n",
    "    def fit(self):\n",
    "        # We define as many predicted scores as lines in the training set\n",
    "        predicted_scores = np.zeros(len(self.training_data))\n",
    "        # We define our dictionary whose keys are the requests and the values the places of the documents\n",
    "        # in the list of documents (the 1 because the second value of the list for each document has the second place the request)\n",
    "        query_indexes = group_queries(self.training_data, 1)\n",
    "        # We retrieve the list of requests\n",
    "        query_keys = query_indexes.keys()\n",
    "        # We obtain here a list of lists of scores, by request\n",
    "        true_scores = [self.training_data[query_indexes[query], 0] for query in query_keys]\n",
    "        # We obtain here the pairs of documents for each list of scores, so for each request\n",
    "        good_ij_pairs = get_pairs(true_scores)\n",
    "        # Here we obtain the set of feature vectors for documents\n",
    "        tree_data = pd.DataFrame(self.training_data[:, 2:7])\n",
    "        # We obtain here all the scores for the documents (the labels)\n",
    "        labels = self.training_data[:, 0]\n",
    "        # The list of dcg is calculated for the lists of scores ranked in descending order (ideal ranking)\n",
    "        # We get a list of lists again\n",
    "        idcg = [ideal_dcg(scores) for scores in true_scores]\n",
    "        # We go through the number of trees \n",
    "        for k in range(self.number_of_trees):\n",
    "            # We create as many lambdas as we have lines in our training set\n",
    "            lambdas = np.zeros(len(predicted_scores))\n",
    "            # We create as much weight as we have lines in our training set\n",
    "            w = np.zeros(len(predicted_scores))\n",
    "            # Here we obtain a list of predicted scores lists, by query\n",
    "            pred_scores = [predicted_scores[query_indexes[query]] for query in query_keys]\n",
    "            # pool allows to parrallelize already on the 4 hearts of the computer, by request\n",
    "            # Here, we obtain the lambdas, the weights for the documents of each request\n",
    "            # In input of compute_lambda, one has the lists of the real scores for the docs of each request,\n",
    "            # lists of predicted scores, lists of pairs of docs such as the first element has a higher score than the second,\n",
    "            # the list of the dcg of the ideal classification of the documents, by request\n",
    "            for lambda_val, w_val, query_key in map(compute_lambda, zip(true_scores, pred_scores, good_ij_pairs, idcg, query_keys)):\n",
    "                # Here, we obtain the list of the places of the documents associated with the request \n",
    "                indexes = query_indexes[query_key]\n",
    "                # We get the lambdas of the docs associated with the input request, we enter the final lambda vector\n",
    "                lambdas[indexes] = lambda_val\n",
    "                # We obtain the weights of the docs associated with the input request, we enter the vector final weight\n",
    "                w[indexes] = w_val\n",
    "            # Implementation sklearn of the three\n",
    "            tree = DecisionTreeRegressor(max_depth=50)\n",
    "            # We made the lambdas tree\n",
    "            tree.fit(self.training_data[:,2:], lambdas)\n",
    "            # We add the tree to our list of trees\n",
    "            self.trees.append(tree)\n",
    "            # We predict thanks to our tree and our features (the predict function is below)\n",
    "            # Calculate the prediction for each document thanks to our previous trees and the new\n",
    "            prediction = tree.predict(self.training_data[:,2:])\n",
    "            # We update our predictions using our recent prediciton and learning_rate\n",
    "            predicted_scores += prediction * self.learning_rate\n",
    "\n",
    "    \n",
    "    def predict(self, data):\n",
    "        data = np.array(data)\n",
    "        query_indexes = group_queries(data, 0)\n",
    "        predicted_scores = np.zeros(len(data))\n",
    "        for query in query_indexes:\n",
    "            results = np.zeros(len(query_indexes[query]))\n",
    "            for tree in self.trees:\n",
    "                results += self.learning_rate * tree.predict(data[query_indexes[query], 1:])\n",
    "            predicted_scores[query_indexes[query]] = results\n",
    "        return predicted_scores\n",
    "\n",
    "    #Validate function that predicts on the test base according to all the trees we have built, which also calculates the NDCG average\n",
    "    def validate(self, data, k):\n",
    "        data = np.array(data)\n",
    "        query_indexes = group_queries(data, 1)\n",
    "        average_ndcg = []\n",
    "        predicted_scores = np.zeros(len(data))\n",
    "        for query in query_indexes:\n",
    "            results = np.zeros(len(query_indexes[query]))\n",
    "            for tree in self.trees:\n",
    "                results += self.learning_rate * tree.predict(data[query_indexes[query], 2:])\n",
    "            predicted_sorted_indexes = np.argsort(results)[::-1]\n",
    "            t_results = data[query_indexes[query], 0]\n",
    "            t_results = t_results[predicted_sorted_indexes]\n",
    "            predicted_scores[query_indexes[query]] = results\n",
    "            dcg_val = dcg_k(t_results, k)\n",
    "            idcg_val = ideal_dcg_k(t_results, k)\n",
    "            ndcg_val = (dcg_val / idcg_val)\n",
    "            average_ndcg.append(ndcg_val)\n",
    "        average_ndcg = np.nanmean(average_ndcg)\n",
    "        return average_ndcg, predicted_scores\n",
    "\n",
    "def group_queries(training_data, qid_index):\n",
    "    # We create a dictionary whose keys are the queries and the values the lists of places of the documents in the list\n",
    "    # documents used in the train database (list of lists containing the query the score and the features associated with the documents)\n",
    "    query_indexes = {}\n",
    "    index = 0\n",
    "    for record in training_data:\n",
    "        query_indexes.setdefault(record[qid_index], [])\n",
    "        query_indexes[record[qid_index]].append(index)\n",
    "        index += 1\n",
    "    return query_indexes\n",
    " \n",
    "# At the input of the function, we have a list of score lists according to the request\n",
    "def get_pairs(scores):\n",
    "    # We define a list \n",
    "    query_pair = []\n",
    "    # Here we go through the lists in the list\n",
    "    for query_scores in scores:\n",
    "        # We sort the scores in descending order (highest scores in premiere)\n",
    "        temp = sorted(query_scores, reverse=True)\n",
    "        pairs = []\n",
    "        for i in range(len(temp)):\n",
    "            for j in range(len(temp)):\n",
    "                if temp[i] > temp[j]:\n",
    "                    pairs.append((i,j))\n",
    "        # We add to the final list \n",
    "        query_pair.append(pairs)\n",
    "    return query_pair\n",
    "\n",
    "# We define the LambdaMART class\n",
    "class LambdaMART:\n",
    "\n",
    "    # On fixed 5 trees and a learning rate of 0.1 .. we can test other learning rate after !!\n",
    "    def __init__(self, training_data=None, number_of_trees=5, learning_rate=0.1):\n",
    "\n",
    "        self.training_data = training_data\n",
    "        self.number_of_trees = number_of_trees\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "    \n",
    "    #The function fit allows to fitter the lambdas tree\n",
    "    def fit(self):\n",
    "        # We define as many predicted scores as lines in the training set\n",
    "        predicted_scores = np.zeros(len(self.training_data))\n",
    "        # We define our dictionary whose keys are the requests and the values the places of the documents\n",
    "        # in the list of documents (the 1 because the second value of the list for each document has the second place the request)\n",
    "        query_indexes = group_queries(self.training_data, 1)\n",
    "        # We retrieve the list of requests\n",
    "        query_keys = query_indexes.keys()\n",
    "        # We obtain here a list of lists of scores, by request\n",
    "        true_scores = [self.training_data[query_indexes[query], 0] for query in query_keys]\n",
    "        # We obtain here the pairs of documents for each list of scores, so for each request\n",
    "        good_ij_pairs = get_pairs(true_scores)\n",
    "        # Here we obtain the set of feature vectors for documents\n",
    "        tree_data = pd.DataFrame(self.training_data[:, 2:7])\n",
    "        # We obtain here all the scores for the documents (the labels)\n",
    "        labels = self.training_data[:, 0]\n",
    "        # The list of dcg is calculated for the lists of scores ranked in descending order (ideal ranking)\n",
    "        # We get a list of lists again\n",
    "        idcg = [ideal_dcg(scores) for scores in true_scores]\n",
    "        # We go through the number of trees \n",
    "        for k in range(self.number_of_trees):\n",
    "            # We create as many lambdas as we have lines in our training set\n",
    "            lambdas = np.zeros(len(predicted_scores))\n",
    "            # We create as much weight as we have lines in our training set\n",
    "            w = np.zeros(len(predicted_scores))\n",
    "            # Here we obtain a list of predicted scores lists, by query\n",
    "            pred_scores = [predicted_scores[query_indexes[query]] for query in query_keys]\n",
    "            # pool allows to parrallelize already on the 4 hearts of the computer, by request\n",
    "            # Here, we obtain the lambdas, the weights for the documents of each request\n",
    "            # In input of compute_lambda, one has the lists of the real scores for the docs of each request,\n",
    "            # lists of predicted scores, lists of pairs of docs such as the first element has a higher score than the second,\n",
    "            # the list of the dcg of the ideal classification of the documents, by request\n",
    "            for lambda_val, w_val, query_key in map(compute_lambda, zip(true_scores, pred_scores, good_ij_pairs, idcg, query_keys)):\n",
    "                # Here, we obtain the list of the places of the documents associated with the request \n",
    "                indexes = query_indexes[query_key]\n",
    "                # We get the lambdas of the docs associated with the input request, we enter the final lambda vector\n",
    "                lambdas[indexes] = lambda_val\n",
    "                # We obtain the weights of the docs associated with the input request, we enter the vector final weight\n",
    "                w[indexes] = w_val\n",
    "            # Implementation sklearn of the three\n",
    "            tree = DecisionTreeRegressor(max_depth=50)\n",
    "            # We made the lambdas tree\n",
    "            tree.fit(self.training_data[:,2:], lambdas)\n",
    "            # We add the tree to our list of trees\n",
    "            self.trees.append(tree)\n",
    "            # We predict thanks to our tree and our features (the predict function is below)\n",
    "            # Calcule la prédiction pour chaque document grâce à nos précédents arbres et au nouveau\n",
    "            prediction = tree.predict(self.training_data[:,2:])\n",
    "            # We update our predictions using our recent prediciton and learning_rate\n",
    "            predicted_scores += prediction * self.learning_rate\n",
    "\n",
    "    \n",
    "    def predict(self, data):\n",
    "        data = np.array(data)\n",
    "        query_indexes = group_queries(data, 0)\n",
    "        predicted_scores = np.zeros(len(data))\n",
    "        for query in query_indexes:\n",
    "            results = np.zeros(len(query_indexes[query]))\n",
    "            for tree in self.trees:\n",
    "                results += self.learning_rate * tree.predict(data[query_indexes[query], 1:])\n",
    "            predicted_scores[query_indexes[query]] = results\n",
    "        return predicted_scores\n",
    "\n",
    "    #Validate function that predicts on the test base according to all the trees we have built, which also calculates the NDCG average\n",
    "    def validate(self, data, k):\n",
    "        data = np.array(data)\n",
    "        query_indexes = group_queries(data, 1)\n",
    "        average_ndcg = []\n",
    "        predicted_scores = np.zeros(len(data))\n",
    "        for query in query_indexes:\n",
    "            results = np.zeros(len(query_indexes[query]))\n",
    "            for tree in self.trees:\n",
    "                results += self.learning_rate * tree.predict(data[query_indexes[query], 2:])\n",
    "            predicted_sorted_indexes = np.argsort(results)[::-1]\n",
    "            t_results = data[query_indexes[query], 0]\n",
    "            t_results = t_results[predicted_sorted_indexes]\n",
    "            predicted_scores[query_indexes[query]] = results\n",
    "            dcg_val = dcg_k(t_results, k)\n",
    "            idcg_val = ideal_dcg_k(t_results, k)\n",
    "            ndcg_val = (dcg_val / idcg_val)\n",
    "            average_ndcg.append(ndcg_val)\n",
    "        average_ndcg = np.nanmean(average_ndcg)\n",
    "        return average_ndcg, predicted_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.793643251190486\n",
      "--- 13.553044796 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    total_ndcg = 0.0\n",
    "    df=pd.read_csv('train2.csv')\n",
    "    values=[[df.loc[(df.index == j),][str(u)][j] for u in list(range(1,49))] for j in range(len(df.index))][0:18]\n",
    "    training_data = np.asarray(values)\n",
    "    df2=pd.read_csv('test2.csv')\n",
    "    values2=[[df2.loc[(df2.index == j),][str(u)][j] for u in list(range(1,49))] for j in range(len(df2.index))][0:18]\n",
    "    test_data = np.asarray(values2)\n",
    "    # We build the 100 trees on the test base\n",
    "    model = LambdaMART(training_data, 100, 0.001)\n",
    "    model.fit()\n",
    "    # We truncate on the first 10 docs for the calculations of the DCG average\n",
    "    # Predicted scores are calculated on the test basis\n",
    "    average_ndcg, predicted_scores = model.validate(test_data, 20) \n",
    "    print (average_ndcg)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
